{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiMPM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UNTm_4w8l7lX","colab_type":"text"},"source":["# Bilateral Multi-Perspective Matching for Natural Language Sentences\n","\n","One of the non-BERT state of the art models used in sentence pair classification. The [paper](https://arxiv.org/pdf/1702.03814.pdf://) explains the functionality of the model. Essentially, it is an extension of an LSTM that works in two matchings (thus the prefix 'Bi' in the name).\n","\n","![BiMPM structure](https://www2.stetson.edu/mathcs/wp-content/uploads/2017/10/Matrix.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R8G9FiHMpat5","colab_type":"text"},"source":["### Mounting Google Drive\n"]},{"cell_type":"code","metadata":{"id":"PMrswJ6qoWxJ","colab_type":"code","outputId":"159c7ace-fa39-4d7d-a427-7d815467e6aa","executionInfo":{"status":"ok","timestamp":1575654932832,"user_tz":300,"elapsed":33109,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7QjI9rA0tWzT","colab_type":"code","outputId":"1f8bbede-fe9e-4910-c1a9-b453230c2129","executionInfo":{"status":"ok","timestamp":1575654937075,"user_tz":300,"elapsed":37331,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5X-UCaQ5tcCZ","colab_type":"code","outputId":"364c0401-f4ca-4e61-ba2b-9ca437936567","executionInfo":{"status":"ok","timestamp":1575654938540,"user_tz":300,"elapsed":38773,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd '/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GujBYUb9pjbw","colab_type":"text"},"source":["### Training\n"]},{"cell_type":"code","metadata":{"id":"OB4m2BAXwdVO","colab_type":"code","outputId":"defa269a-1012-4a28-c99c-7f77da25418a","executionInfo":{"status":"ok","timestamp":1575654944717,"user_tz":300,"elapsed":44929,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":248}},"source":["!pip install tensorboardX\n","import nltk\n","nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n","\r\u001b[K     |█▊                              | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 41.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51kB 42.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61kB 45.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 35.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 81kB 35.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 92kB 37.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 102kB 35.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112kB 35.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 122kB 35.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 133kB 35.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 143kB 35.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 153kB 35.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 163kB 35.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 174kB 35.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 184kB 35.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 35.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (42.0.1)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-1.9\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"3EvYbvSb6MX-","colab_type":"text"},"source":["#### utils.py\n"]},{"cell_type":"code","metadata":{"id":"ucvyYAcY8Ekh","colab_type":"code","colab":{}},"source":["%pycat model/utils.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_CE_qlSAFXA","colab_type":"code","outputId":"5d59ba59-9d8c-44fe-ecc9-e8df74d81fb6","executionInfo":{"status":"ok","timestamp":1575654945728,"user_tz":300,"elapsed":45894,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile model/utils.py\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.vocab import GloVe, Vectors, FastText\n","\n","import torch\n","\n","from nltk import word_tokenize\n","\n","\n","class SNLI():\n","    def __init__(self, args):\n","        self.TEXT = data.Field(batch_first=True, tokenize=word_tokenize, lower=True)\n","        self.LABEL = data.Field(sequential=False, unk_token=None)\n","\n","        self.train, self.dev, self.test = datasets.SNLI.splits(self.TEXT, self.LABEL)\n","\n","        self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n","        self.LABEL.build_vocab(self.train)\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.train_iter, self.dev_iter, self.test_iter = \\\n","            data.BucketIterator.splits((self.train, self.dev, self.test),\n","                                       batch_sizes=[args.batch_size] * 3,\n","                                       device=device)\n","\n","        self.max_word_len = max([len(w) for w in self.TEXT.vocab.itos])\n","        # for <pad>\n","        self.char_vocab = {'': 0}\n","        # for <unk> and <pad>\n","        self.characterized_words = [[0] * self.max_word_len, [0] * self.max_word_len]\n","\n","        if args.use_char_emb:\n","            self.build_char_vocab()\n","\n","    def build_char_vocab(self):\n","        # for normal words\n","        for word in self.TEXT.vocab.itos[2:]:\n","            chars = []\n","            for c in list(word):\n","                if c not in self.char_vocab:\n","                    self.char_vocab[c] = len(self.char_vocab)\n","\n","                chars.append(self.char_vocab[c])\n","\n","            chars.extend([0] * (self.max_word_len - len(word)))\n","            self.characterized_words.append(chars)\n","\n","    def characterize(self, batch):\n","        \"\"\"\n","        :param batch: Pytorch Variable with shape (batch, seq_len)\n","        :return: Pytorch Variable with shape (batch, seq_len, max_word_len)\n","        \"\"\"\n","        batch = batch.data.cpu().numpy().astype(int).tolist()\n","        return [[self.characterized_words[w] for w in words] for words in batch]\n","\n","class Quora():\n","    def __init__(self, args):\n","        self.RAW = data.RawField()\n","        self.RAW.is_target = False\n","        self.TEXT = data.Field(batch_first=True)\n","        self.LABEL = data.Field(sequential=False, unk_token=None)\n","\n","        self.train, self.dev, self.test = data.TabularDataset.splits(\n","            path='.data/quora',\n","            train='train.tsv',\n","            validation='dev.tsv',\n","            test='test.tsv',\n","            format='tsv',\n","            fields=[('label', self.LABEL),\n","                    ('q1', self.TEXT),\n","                    ('q2', self.TEXT),\n","                    ('id', self.RAW)])\n","\n","        self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n","        self.LABEL.build_vocab(self.train)\n","\n","        sort_key = lambda x: data.interleave_keys(len(x.q1), len(x.q2))\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.train_iter, self.dev_iter, self.test_iter = \\\n","            data.BucketIterator.splits((self.train, self.dev, self.test),\n","                                       batch_sizes=[args.batch_size] * 3,\n","                                       device=device,\n","                                       sort_key=sort_key)\n","\n","        self.max_word_len = max([len(w) for w in self.TEXT.vocab.itos])\n","        # for <pad>\n","        self.char_vocab = {'': 0}\n","        # for <unk> and <pad>\n","        self.characterized_words = [[0] * self.max_word_len, [0] * self.max_word_len]\n","\n","        if args.use_char_emb:\n","            self.build_char_vocab()\n","\n","    def build_char_vocab(self):\n","        # for normal words\n","        for word in self.TEXT.vocab.itos[2:]:\n","            chars = []\n","            for c in list(word):\n","                if c not in self.char_vocab:\n","                    self.char_vocab[c] = len(self.char_vocab)\n","\n","                chars.append(self.char_vocab[c])\n","\n","            chars.extend([0] * (self.max_word_len - len(word)))\n","            self.characterized_words.append(chars)\n","\n","    def characterize(self, batch):\n","        \"\"\"\n","        :param batch: Pytorch Variable with shape (batch, seq_len)\n","        :return: Pytorch Variable with shape (batch, seq_len, max_word_len)\n","        \"\"\"\n","        batch = batch.data.cpu().numpy().astype(int).tolist()\n","        return [[self.characterized_words[w] for w in words] for words in batch]\n","\n","class BQ():\n","    def __init__(self, args):\n","        self.RAW = data.RawField()\n","        self.RAW.is_target = False\n","        tokenize = lambda x : list(x)\n","        self.TEXT = data.Field(batch_first=True, tokenize=tokenize)\n","        self.LABEL = data.Field(sequential=False, unk_token=None)\n","\n","        self.train, self.dev, self.test = data.TabularDataset.splits(\n","            path='dataset/sent_pair/bq',\n","            train='train.tsv',\n","            validation='dev.tsv',\n","            test='test.tsv',\n","            format='tsv',\n","            fields=[('label', self.LABEL),\n","                    ('q1', self.TEXT),\n","                    ('q2', self.TEXT),\n","                    ('id', self.RAW)])\n","\n","        self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=FastText(language='zh'))\n","        self.LABEL.build_vocab(self.train)\n","\n","        sort_key = lambda x: data.interleave_keys(len(x.q1), len(x.q2))\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.train_iter, self.dev_iter, self.test_iter = \\\n","            data.BucketIterator.splits((self.train, self.dev, self.test),\n","                                       batch_sizes=[args.batch_size] * 3,\n","                                       device=device,\n","                                       sort_key=sort_key)\n","        self.max_word_len = max([len(w) for w in self.TEXT.vocab.itos])\n","        # for <pad>\n","        self.char_vocab = {'': 0}\n","        # for <unk> and <pad>\n","        self.characterized_words = [[0] * self.max_word_len, [0] * self.max_word_len]\n","\n","        if args.use_char_emb:\n","            self.build_char_vocab()\n","\n","    def build_char_vocab(self):\n","        # for normal words\n","        for word in self.TEXT.vocab.itos[2:]:\n","            chars = []\n","            for c in list(word):\n","                if c not in self.char_vocab:\n","                    self.char_vocab[c] = len(self.char_vocab)\n","\n","                chars.append(self.char_vocab[c])\n","\n","            chars.extend([0] * (self.max_word_len - len(word)))\n","            self.characterized_words.append(chars)\n","\n","    def characterize(self, batch):\n","        \"\"\"\n","        :param batch: Pytorch Variable with shape (batch, seq_len)\n","        :return: Pytorch Variable with shape (batch, seq_len, max_word_len)\n","        \"\"\"\n","        batch = batch.data.cpu().numpy().astype(int).tolist()\n","        return [[self.characterized_words[w] for w in words] for words in batch]\n","\n","        "],"execution_count":6,"outputs":[{"output_type":"stream","text":["Overwriting model/utils.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7lwroYWO6DS-","colab_type":"text"},"source":["#### BIMPM.py"]},{"cell_type":"code","metadata":{"id":"BooPt4RO5Oqh","colab_type":"code","colab":{}},"source":["%pycat model/BIMPM.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFjn5-sr5lwV","colab_type":"code","outputId":"3957a190-919a-46e9-e4b3-2857bd1ca561","executionInfo":{"status":"ok","timestamp":1575654946527,"user_tz":300,"elapsed":46653,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile model/BIMPM.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BIMPM(nn.Module):\n","    def __init__(self, args, data):\n","        super(BIMPM, self).__init__()\n","\n","        self.args = args\n","        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n","        self.l = self.args.num_perspective\n","\n","        # ----- Word Representation Layer -----\n","        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n","\n","        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n","        # initialize word embedding with GloVe\n","        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n","        # no fine-tuning for word vectors\n","        self.word_emb.weight.requires_grad = False\n","\n","        self.char_LSTM = nn.LSTM(\n","            input_size=self.args.char_dim,\n","            hidden_size=self.args.char_hidden_size,\n","            num_layers=1,\n","            bidirectional=False,\n","            batch_first=True)\n","\n","        # ----- Context Representation Layer -----\n","        self.context_LSTM = nn.LSTM(\n","            input_size=self.d,\n","            hidden_size=self.args.hidden_size,\n","            num_layers=1,\n","            bidirectional=True,\n","            batch_first=True\n","        )\n","\n","        # ----- Matching Layer -----\n","        for i in range(1, 9):\n","            setattr(self, f'mp_w{i}',\n","                    nn.Parameter(torch.rand(self.l, self.args.hidden_size)))\n","\n","        # ----- Aggregation Layer -----\n","        self.aggregation_LSTM = nn.LSTM(\n","            input_size=self.l * 8,\n","            hidden_size=self.args.hidden_size,\n","            num_layers=1,\n","            bidirectional=True,\n","            batch_first=True\n","        )\n","\n","        # ----- Prediction Layer -----\n","        self.pred_fc1 = nn.Linear(self.args.hidden_size * 4, self.args.hidden_size * 2)\n","        self.pred_fc2 = nn.Linear(self.args.hidden_size * 2, self.args.class_size)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        # ----- Word Representation Layer -----\n","        nn.init.uniform_(self.char_emb.weight, -0.005, 0.005)\n","        # zero vectors for padding\n","        self.char_emb.weight.data[0].fill_(0)\n","\n","        # <unk> vectors is randomly initialized\n","        nn.init.uniform_(self.word_emb.weight.data[0], -0.1, 0.1)\n","\n","        nn.init.kaiming_normal_(self.char_LSTM.weight_ih_l0)\n","        nn.init.constant_(self.char_LSTM.bias_ih_l0, val=0)\n","        nn.init.orthogonal_(self.char_LSTM.weight_hh_l0)\n","        nn.init.constant_(self.char_LSTM.bias_hh_l0, val=0)\n","\n","        # ----- Context Representation Layer -----\n","        nn.init.kaiming_normal_(self.context_LSTM.weight_ih_l0)\n","        nn.init.constant_(self.context_LSTM.bias_ih_l0, val=0)\n","        nn.init.orthogonal_(self.context_LSTM.weight_hh_l0)\n","        nn.init.constant_(self.context_LSTM.bias_hh_l0, val=0)\n","\n","        nn.init.kaiming_normal_(self.context_LSTM.weight_ih_l0_reverse)\n","        nn.init.constant_(self.context_LSTM.bias_ih_l0_reverse, val=0)\n","        nn.init.orthogonal_(self.context_LSTM.weight_hh_l0_reverse)\n","        nn.init.constant_(self.context_LSTM.bias_hh_l0_reverse, val=0)\n","\n","        # ----- Matching Layer -----\n","        for i in range(1, 9):\n","            w = getattr(self, f'mp_w{i}')\n","            nn.init.kaiming_normal_(w)\n","\n","        # ----- Aggregation Layer -----\n","        nn.init.kaiming_normal_(self.aggregation_LSTM.weight_ih_l0)\n","        nn.init.constant_(self.aggregation_LSTM.bias_ih_l0, val=0)\n","        nn.init.orthogonal_(self.aggregation_LSTM.weight_hh_l0)\n","        nn.init.constant_(self.aggregation_LSTM.bias_hh_l0, val=0)\n","\n","        nn.init.kaiming_normal_(self.aggregation_LSTM.weight_ih_l0_reverse)\n","        nn.init.constant_(self.aggregation_LSTM.bias_ih_l0_reverse, val=0)\n","        nn.init.orthogonal_(self.aggregation_LSTM.weight_hh_l0_reverse)\n","        nn.init.constant_(self.aggregation_LSTM.bias_hh_l0_reverse, val=0)\n","\n","        # ----- Prediction Layer ----\n","        nn.init.uniform_(self.pred_fc1.weight, -0.005, 0.005)\n","        nn.init.constant_(self.pred_fc1.bias, val=0)\n","\n","        nn.init.uniform_(self.pred_fc2.weight, -0.005, 0.005)\n","        nn.init.constant_(self.pred_fc2.bias, val=0)\n","\n","    def dropout(self, v):\n","        return F.dropout(v, p=self.args.dropout, training=self.training)\n","\n","    def forward(self, **kwargs):\n","        # ----- Matching Layer -----\n","        def mp_matching_func(v1, v2, w):\n","            \"\"\"\n","            :param v1: (batch, seq_len, hidden_size)\n","            :param v2: (batch, seq_len, hidden_size) or (batch, hidden_size)\n","            :param w: (l, hidden_size)\n","            :return: (batch, l)\n","            \"\"\"\n","            seq_len = v1.size(1)\n","\n","            # Trick for large memory requirement\n","            \"\"\"\n","            if len(v2.size()) == 2:\n","                v2 = torch.stack([v2] * seq_len, dim=1)\n","\n","            m = []\n","            for i in range(self.l):\n","                # v1: (batch, seq_len, hidden_size)\n","                # v2: (batch, seq_len, hidden_size)\n","                # w: (1, 1, hidden_size)\n","                # -> (batch, seq_len)\n","                m.append(F.cosine_similarity(w[i].view(1, 1, -1) * v1, w[i].view(1, 1, -1) * v2, dim=2))\n","\n","            # list of (batch, seq_len) -> (batch, seq_len, l)\n","            m = torch.stack(m, dim=2)\n","            \"\"\"\n","\n","            # (1, 1, hidden_size, l)\n","            w = w.transpose(1, 0).unsqueeze(0).unsqueeze(0)\n","            # (batch, seq_len, hidden_size, l)\n","            v1 = w * torch.stack([v1] * self.l, dim=3)\n","            if len(v2.size()) == 3:\n","                v2 = w * torch.stack([v2] * self.l, dim=3)\n","            else:\n","                v2 = w * torch.stack([torch.stack([v2] * seq_len, dim=1)] * self.l, dim=3)\n","\n","            m = F.cosine_similarity(v1, v2, dim=2)\n","\n","            return m\n","\n","        def mp_matching_func_pairwise(v1, v2, w):\n","            \"\"\"\n","            :param v1: (batch, seq_len1, hidden_size)\n","            :param v2: (batch, seq_len2, hidden_size)\n","            :param w: (l, hidden_size)\n","            :return: (batch, l, seq_len1, seq_len2)\n","            \"\"\"\n","\n","            # Trick for large memory requirement\n","            \"\"\"\n","            m = []\n","            for i in range(self.l):\n","                # (1, 1, hidden_size)\n","                w_i = w[i].view(1, 1, -1)\n","                # (batch, seq_len1, hidden_size), (batch, seq_len2, hidden_size)\n","                v1, v2 = w_i * v1, w_i * v2\n","                # (batch, seq_len, hidden_size->1)\n","                v1_norm = v1.norm(p=2, dim=2, keepdim=True)\n","                v2_norm = v2.norm(p=2, dim=2, keepdim=True)\n","\n","                # (batch, seq_len1, seq_len2)\n","                n = torch.matmul(v1, v2.permute(0, 2, 1))\n","                d = v1_norm * v2_norm.permute(0, 2, 1)\n","\n","                m.append(div_with_small_value(n, d))\n","\n","            # list of (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, l)\n","            m = torch.stack(m, dim=3)\n","            \"\"\"\n","\n","            # (1, l, 1, hidden_size)\n","            w = w.unsqueeze(0).unsqueeze(2)\n","            # (batch, l, seq_len, hidden_size)\n","            v1, v2 = w * torch.stack([v1] * self.l, dim=1), w * torch.stack([v2] * self.l, dim=1)\n","            # (batch, l, seq_len, hidden_size->1)\n","            v1_norm = v1.norm(p=2, dim=3, keepdim=True)\n","            v2_norm = v2.norm(p=2, dim=3, keepdim=True)\n","\n","            # (batch, l, seq_len1, seq_len2)\n","            n = torch.matmul(v1, v2.transpose(2, 3))\n","            d = v1_norm * v2_norm.transpose(2, 3)\n","\n","            # (batch, seq_len1, seq_len2, l)\n","            m = div_with_small_value(n, d).permute(0, 2, 3, 1)\n","\n","            return m\n","\n","        def attention(v1, v2):\n","            \"\"\"\n","            :param v1: (batch, seq_len1, hidden_size)\n","            :param v2: (batch, seq_len2, hidden_size)\n","            :return: (batch, seq_len1, seq_len2)\n","            \"\"\"\n","\n","            # (batch, seq_len1, 1)\n","            v1_norm = v1.norm(p=2, dim=2, keepdim=True)\n","            # (batch, 1, seq_len2)\n","            v2_norm = v2.norm(p=2, dim=2, keepdim=True).permute(0, 2, 1)\n","\n","            # (batch, seq_len1, seq_len2)\n","            a = torch.bmm(v1, v2.permute(0, 2, 1))\n","            d = v1_norm * v2_norm\n","\n","            return div_with_small_value(a, d)\n","\n","        def div_with_small_value(n, d, eps=1e-8):\n","            # too small values are replaced by 1e-8 to prevent it from exploding.\n","            d = d * (d > eps).float() + eps * (d <= eps).float()\n","            return n / d\n","\n","        # ----- Word Representation Layer -----\n","        # (batch, seq_len) -> (batch, seq_len, word_dim)\n","\n","        p = self.word_emb(kwargs['p'])\n","        h = self.word_emb(kwargs['h'])\n","\n","        if self.args.use_char_emb:\n","            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n","            seq_len_p = kwargs['char_p'].size(1)\n","            seq_len_h = kwargs['char_h'].size(1)\n","\n","            char_p = kwargs['char_p'].view(-1, self.args.max_word_len)\n","            char_h = kwargs['char_h'].view(-1, self.args.max_word_len)\n","\n","            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n","            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n","            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n","\n","            # (batch, seq_len, char_hidden_size)\n","            char_p = char_p.view(-1, seq_len_p, self.args.char_hidden_size)\n","            char_h = char_h.view(-1, seq_len_h, self.args.char_hidden_size)\n","\n","            # (batch, seq_len, word_dim + char_hidden_size)\n","            p = torch.cat([p, char_p], dim=-1)\n","            h = torch.cat([h, char_h], dim=-1)\n","\n","        p = self.dropout(p)\n","        h = self.dropout(h)\n","\n","        # ----- Context Representation Layer -----\n","        # (batch, seq_len, hidden_size * 2)\n","        con_p, _ = self.context_LSTM(p)\n","        con_h, _ = self.context_LSTM(h)\n","\n","        con_p = self.dropout(con_p)\n","        con_h = self.dropout(con_h)\n","\n","        # (batch, seq_len, hidden_size)\n","        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n","        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n","\n","        # 1. Full-Matching\n","\n","        # (batch, seq_len, hidden_size), (batch, hidden_size)\n","        # -> (batch, seq_len, l)\n","        mv_p_full_fw = mp_matching_func(con_p_fw, con_h_fw[:, -1, :], self.mp_w1)\n","        mv_p_full_bw = mp_matching_func(con_p_bw, con_h_bw[:, 0, :], self.mp_w2)\n","        mv_h_full_fw = mp_matching_func(con_h_fw, con_p_fw[:, -1, :], self.mp_w1)\n","        mv_h_full_bw = mp_matching_func(con_h_bw, con_p_bw[:, 0, :], self.mp_w2)\n","\n","        # 2. Maxpooling-Matching\n","\n","        # (batch, seq_len1, seq_len2, l)\n","        mv_max_fw = mp_matching_func_pairwise(con_p_fw, con_h_fw, self.mp_w3)\n","        mv_max_bw = mp_matching_func_pairwise(con_p_bw, con_h_bw, self.mp_w4)\n","\n","        # (batch, seq_len, l)\n","        mv_p_max_fw, _ = mv_max_fw.max(dim=2)\n","        mv_p_max_bw, _ = mv_max_bw.max(dim=2)\n","        mv_h_max_fw, _ = mv_max_fw.max(dim=1)\n","        mv_h_max_bw, _ = mv_max_bw.max(dim=1)\n","\n","        # 3. Attentive-Matching\n","\n","        # (batch, seq_len1, seq_len2)\n","        att_fw = attention(con_p_fw, con_h_fw)\n","        att_bw = attention(con_p_bw, con_h_bw)\n","\n","        # (batch, seq_len2, hidden_size) -> (batch, 1, seq_len2, hidden_size)\n","        # (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, 1)\n","        # -> (batch, seq_len1, seq_len2, hidden_size)\n","        att_h_fw = con_h_fw.unsqueeze(1) * att_fw.unsqueeze(3)\n","        att_h_bw = con_h_bw.unsqueeze(1) * att_bw.unsqueeze(3)\n","        # (batch, seq_len1, hidden_size) -> (batch, seq_len1, 1, hidden_size)\n","        # (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, 1)\n","        # -> (batch, seq_len1, seq_len2, hidden_size)\n","        att_p_fw = con_p_fw.unsqueeze(2) * att_fw.unsqueeze(3)\n","        att_p_bw = con_p_bw.unsqueeze(2) * att_bw.unsqueeze(3)\n","\n","        # (batch, seq_len1, hidden_size) / (batch, seq_len1, 1) -> (batch, seq_len1, hidden_size)\n","        att_mean_h_fw = div_with_small_value(att_h_fw.sum(dim=2), att_fw.sum(dim=2, keepdim=True))\n","        att_mean_h_bw = div_with_small_value(att_h_bw.sum(dim=2), att_bw.sum(dim=2, keepdim=True))\n","\n","        # (batch, seq_len2, hidden_size) / (batch, seq_len2, 1) -> (batch, seq_len2, hidden_size)\n","        att_mean_p_fw = div_with_small_value(att_p_fw.sum(dim=1), att_fw.sum(dim=1, keepdim=True).permute(0, 2, 1))\n","        att_mean_p_bw = div_with_small_value(att_p_bw.sum(dim=1), att_bw.sum(dim=1, keepdim=True).permute(0, 2, 1))\n","\n","        # (batch, seq_len, l)\n","        mv_p_att_mean_fw = mp_matching_func(con_p_fw, att_mean_h_fw, self.mp_w5)\n","        mv_p_att_mean_bw = mp_matching_func(con_p_bw, att_mean_h_bw, self.mp_w6)\n","        mv_h_att_mean_fw = mp_matching_func(con_h_fw, att_mean_p_fw, self.mp_w5)\n","        mv_h_att_mean_bw = mp_matching_func(con_h_bw, att_mean_p_bw, self.mp_w6)\n","\n","        # 4. Max-Attentive-Matching\n","\n","        # (batch, seq_len1, hidden_size)\n","        att_max_h_fw, _ = att_h_fw.max(dim=2)\n","        att_max_h_bw, _ = att_h_bw.max(dim=2)\n","        # (batch, seq_len2, hidden_size)\n","        att_max_p_fw, _ = att_p_fw.max(dim=1)\n","        att_max_p_bw, _ = att_p_bw.max(dim=1)\n","\n","        # (batch, seq_len, l)\n","        mv_p_att_max_fw = mp_matching_func(con_p_fw, att_max_h_fw, self.mp_w7)\n","        mv_p_att_max_bw = mp_matching_func(con_p_bw, att_max_h_bw, self.mp_w8)\n","        mv_h_att_max_fw = mp_matching_func(con_h_fw, att_max_p_fw, self.mp_w7)\n","        mv_h_att_max_bw = mp_matching_func(con_h_bw, att_max_p_bw, self.mp_w8)\n","\n","        # (batch, seq_len, l * 8)\n","        mv_p = torch.cat(\n","            [mv_p_full_fw, mv_p_max_fw, mv_p_att_mean_fw, mv_p_att_max_fw,\n","             mv_p_full_bw, mv_p_max_bw, mv_p_att_mean_bw, mv_p_att_max_bw], dim=2)\n","        mv_h = torch.cat(\n","            [mv_h_full_fw, mv_h_max_fw, mv_h_att_mean_fw, mv_h_att_max_fw,\n","             mv_h_full_bw, mv_h_max_bw, mv_h_att_mean_bw, mv_h_att_max_bw], dim=2)\n","\n","        mv_p = self.dropout(mv_p)\n","        mv_h = self.dropout(mv_h)\n","\n","        # ----- Aggregation Layer -----\n","        # (batch, seq_len, l * 8) -> (2, batch, hidden_size)\n","        _, (agg_p_last, _) = self.aggregation_LSTM(mv_p)\n","        _, (agg_h_last, _) = self.aggregation_LSTM(mv_h)\n","\n","        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n","        x = torch.cat(\n","            [agg_p_last.permute(1, 0, 2).contiguous().view(-1, self.args.hidden_size * 2),\n","             agg_h_last.permute(1, 0, 2).contiguous().view(-1, self.args.hidden_size * 2)], dim=1)\n","        x = self.dropout(x)\n","\n","        # ----- Prediction Layer -----\n","        x = torch.tanh(self.pred_fc1(x))\n","        x = self.dropout(x)\n","        x = self.pred_fc2(x)\n","\n","        return x"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Overwriting model/BIMPM.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CZ0toa4U5-mu","colab_type":"text"},"source":["#### train.py\n"]},{"cell_type":"code","metadata":{"id":"o0VZag1h503n","colab_type":"code","colab":{}},"source":["%pycat train.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8rdHoo7544Q","colab_type":"code","outputId":"71e7fce5-4f88-4a58-9239-2cba5b82a45a","executionInfo":{"status":"ok","timestamp":1575654947598,"user_tz":300,"elapsed":47683,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile train.py\n","import argparse\n","import copy\n","import os\n","import torch\n","\n","from torch import nn, optim\n","from torch.autograd import Variable\n","from tensorboardX import SummaryWriter\n","from time import gmtime, strftime\n","\n","from model.BIMPM import BIMPM\n","from model.utils import SNLI, Quora, BQ\n","from test import test\n","\n","\n","def train(args, data):\n","    model = BIMPM(args, data)\n","    if args.gpu > -1:\n","        model.cuda()\n","\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n","\n","    model.train()\n","    loss, last_epoch = 0, -1\n","    max_dev_acc, max_test_acc = 0, 0\n","\n","    iterator = data.train_iter\n","    for i, batch in enumerate(iterator):\n","        present_epoch = int(iterator.epoch)\n","        if present_epoch == args.epoch:\n","            break\n","        if present_epoch > last_epoch:\n","            print('epoch:', present_epoch + 1)\n","        last_epoch = present_epoch\n","\n","        if args.data_type == 'SNLI':\n","            s1, s2 = 'premise', 'hypothesis'\n","        else:\n","            s1, s2 = 'q1', 'q2'\n","\n","        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n","\n","        # limit the lengths of input sentences up to max_sent_len\n","        if args.max_sent_len >= 0:\n","            if s1.size()[1] > args.max_sent_len:\n","                s1 = s1[:, :args.max_sent_len]\n","            if s2.size()[1] > args.max_sent_len:\n","                s2 = s2[:, :args.max_sent_len]\n","\n","        kwargs = {'p': s1, 'h': s2}\n","\n","        if args.use_char_emb:\n","            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n","            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n","\n","            if args.gpu > -1:\n","                char_p = char_p.cuda()\n","                char_h = char_h.cuda()\n","\n","            kwargs['char_p'] = char_p\n","            kwargs['char_h'] = char_h\n","\n","        pred = model(**kwargs)\n","\n","        optimizer.zero_grad()\n","        batch_loss = criterion(pred, batch.label)\n","        loss += batch_loss.item()\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % args.print_freq == 0:\n","            dev_loss, dev_acc = test(model, args, data, mode='dev')\n","            test_loss, test_acc = test(model, args, data)\n","            c = (i + 1) // args.print_freq\n","\n","            writer.add_scalar('loss/train', loss, c)\n","            writer.add_scalar('loss/dev', dev_loss, c)\n","            writer.add_scalar('acc/dev', dev_acc, c)\n","            writer.add_scalar('loss/test', test_loss, c)\n","            writer.add_scalar('acc/test', test_acc, c)\n","\n","            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f} / test loss: {test_loss:.3f}'\n","                  f' / dev acc: {dev_acc:.3f} / test acc: {test_acc:.3f}')\n","\n","            if dev_acc > max_dev_acc:\n","                max_dev_acc = dev_acc\n","                max_test_acc = test_acc\n","                best_model = copy.deepcopy(model)\n","\n","            loss = 0\n","            model.train()\n","\n","    writer.close()\n","    print(f'max dev acc: {max_dev_acc:.3f} / max test acc: {max_test_acc:.3f}')\n","\n","    return best_model\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--batch-size', default=64, type=int)\n","    parser.add_argument('--char-dim', default=20, type=int)\n","    parser.add_argument('--char-hidden-size', default=50, type=int)\n","    parser.add_argument('--data-type', default='SNLI', help='available: SNLI or Quora')\n","    parser.add_argument('--dropout', default=0.1, type=float)\n","    parser.add_argument('--epoch', default=10, type=int)\n","    parser.add_argument('--gpu', default=0, type=int)\n","    parser.add_argument('--hidden-size', default=100, type=int)\n","    parser.add_argument('--learning-rate', default=0.001, type=float)\n","    parser.add_argument('--max-sent-len', default=-1, type=int,\n","                        help='max length of input sentences model can accept, if -1, it accepts any length')\n","    parser.add_argument('--num-perspective', default=20, type=int)\n","    parser.add_argument('--print-freq', default=500, type=int)\n","    parser.add_argument('--use-char-emb', default=True, action='store_true')\n","    parser.add_argument('--word-dim', default=300, type=int)\n","    args = parser.parse_args()\n","\n","    if args.data_type == 'SNLI':\n","        print('loading SNLI data...')\n","        data = SNLI(args)\n","    elif args.data_type == 'Quora':\n","        print('loading Quora data...')\n","        data = Quora(args)\n","    elif args.data_type == 'BQ':\n","        print('loading BQ data...')\n","        data = BQ(args)\n","    else:\n","        raise NotImplementedError('only SNLI or Quora or BQ data is possible')\n","\n","    setattr(args, 'char_vocab_size', len(data.char_vocab))\n","    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n","    setattr(args, 'class_size', len(data.LABEL.vocab))\n","    setattr(args, 'max_word_len', data.max_word_len)\n","    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n","\n","    print('training start!')\n","    best_model = train(args, data)\n","\n","    if not os.path.exists('saved_models'):\n","        os.makedirs('saved_models')\n","    torch.save(best_model.state_dict(), f'saved_models/BIBPM_{args.data_type}_{args.model_time}.pt')\n","\n","    print('training finished!')\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Overwriting train.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_VIQtq1O7zGR","colab_type":"text"},"source":["#### test.py\n","\n"]},{"cell_type":"code","metadata":{"id":"_nfPu4oQ777A","colab_type":"code","colab":{}},"source":["%pycat test.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n15jalJH8gl_","colab_type":"code","outputId":"65125b38-2b0a-456d-96d7-762ec1572e65","executionInfo":{"status":"ok","timestamp":1575654948111,"user_tz":300,"elapsed":48150,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile test.py\n","import argparse\n","\n","import torch\n","from torch import nn\n","from torch.autograd import Variable\n","\n","from model.BIMPM import BIMPM\n","from model.utils import SNLI, Quora, BQ\n","\n","\n","def test(model, args, data, mode='test'):\n","    if mode == 'dev':\n","        iterator = iter(data.dev_iter)\n","    else:\n","        iterator = iter(data.test_iter)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    acc, loss, size = 0, 0, 0\n","\n","    for batch in iterator:\n","        if args.data_type == 'SNLI':\n","            s1, s2 = 'premise', 'hypothesis'\n","        else:\n","            s1, s2 = 'q1', 'q2'\n","\n","        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n","        kwargs = {'p': s1, 'h': s2}\n","\n","        if args.use_char_emb:\n","            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n","            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n","\n","            if args.gpu > -1:\n","                char_p = char_p.cuda()\n","                char_h = char_h.cuda()\n","\n","            kwargs['char_p'] = char_p\n","            kwargs['char_h'] = char_h\n","\n","        pred = model(**kwargs)\n","\n","        batch_loss = criterion(pred, batch.label)\n","        loss += batch_loss.item()\n","\n","        _, pred = pred.max(dim=1)\n","        acc += (pred == batch.label).sum().float()\n","        size += len(pred)\n","\n","    acc /= size\n","    acc = acc.cpu().item()\n","    return loss, acc\n","\n","\n","def load_model(args, data):\n","    model = BIMPM(args, data)\n","    model.load_state_dict(torch.load(args.model_path))\n","\n","    if args.gpu > -1:\n","        model.cuda()\n","\n","    return model\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--batch-size', default=64, type=int)\n","    parser.add_argument('--char-dim', default=20, type=int)\n","    parser.add_argument('--char-hidden-size', default=50, type=int)\n","    parser.add_argument('--dropout', default=0.1, type=float)\n","    parser.add_argument('--data-type', default='SNLI', help='available: SNLI or Quora')\n","    parser.add_argument('--epoch', default=10, type=int)\n","    parser.add_argument('--gpu', default=0, type=int)\n","    parser.add_argument('--hidden-size', default=100, type=int)\n","    parser.add_argument('--learning-rate', default=0.001, type=float)\n","    parser.add_argument('--num-perspective', default=20, type=int)\n","    parser.add_argument('--use-char-emb', default=True, action='store_true')\n","    parser.add_argument('--word-dim', default=300, type=int)\n","\n","    parser.add_argument('--model-path', required=True)\n","\n","    args = parser.parse_args()\n","\n","    if args.data_type == 'SNLI':\n","        print('loading SNLI data...')\n","        data = SNLI(args)\n","    elif args.data_type == 'Quora':\n","        print('loading Quora data...')\n","        data = Quora(args)\n","    elif args.data_type == 'BQ':\n","        print('loading BQ data...')\n","        data = BQ(args)\n","\n","    setattr(args, 'char_vocab_size', len(data.char_vocab))\n","    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n","    setattr(args, 'class_size', len(data.LABEL.vocab))\n","    setattr(args, 'max_word_len', data.max_word_len)\n","\n","    print('loading model...')\n","    model = load_model(args, data)\n","\n","    _, acc = test(model, args, data)\n","\n","    print(f'test acc: {acc:.3f}')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Overwriting test.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjyL5L9Ffoys","colab_type":"code","colab":{}},"source":["import torch\n","torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0WZA7EWpu0A","colab_type":"code","outputId":"4c52645b-5312-4767-8e02-5b2a519e5020","executionInfo":{"status":"ok","timestamp":1575655517798,"user_tz":300,"elapsed":180300,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":582}},"source":["!python train.py --data-type Quora"],"execution_count":14,"outputs":[{"output_type":"stream","text":["loading Quora data...\n","tcmalloc: large alloc 2635227136 bytes == 0x63fe000 @  0x7f2a50a83b6b 0x7f2a50aa3379 0x7f29fd1e62ea 0x7f29fd1e7d9a 0x7f29ff3f9c1b 0x7f29ff5de6fb 0x7f29ff3fa4ed 0x7f29ff3ff345 0x7f29ff724323 0x7f2a4544d294 0x7f2a4537328e 0x50ac25 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50d390 0x508245 0x509642 0x595311 0x5a067e 0x50d966 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x509455\n","100% 2195362/2196017 [03:42<00:00, 10954.71it/s]training start!\n","epoch: 1\n","Traceback (most recent call last):\n","  File \"train.py\", line 151, in <module>\n","    main()\n","  File \"train.py\", line 141, in main\n","    best_model = train(args, data)\n","  File \"train.py\", line 67, in train\n","    pred = model(**kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification/model/BIMPM.py\", line 327, in forward\n","    mv_h_att_max_bw = mp_matching_func(con_h_bw, att_max_p_bw, self.mp_w8)\n","  File \"/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification/model/BIMPM.py\", line 143, in mp_matching_func\n","    v2 = w * torch.stack([v2] * self.l, dim=3)\n","RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.43 GiB total capacity; 6.77 GiB already allocated; 82.94 MiB free; 65.81 MiB cached)\n","Exception ignored in: <bound method tqdm.__del__ of 100% 2195362/2196017 [07:14<00:00, 10954.71it/s]>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 931, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 1133, in close\n","    self._decr_instances(self)\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 496, in _decr_instances\n","    cls.monitor.exit()\n","  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_monitor.py\", line 52, in exit\n","    self.join()\n","  File \"/usr/lib/python3.6/threading.py\", line 1053, in join\n","    raise RuntimeError(\"cannot join current thread\")\n","RuntimeError: cannot join current thread\n"],"name":"stdout"}]}]}
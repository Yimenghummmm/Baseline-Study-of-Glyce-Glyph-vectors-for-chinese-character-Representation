{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextCNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KhJ8RmC2MMYc","colab_type":"code","outputId":"656f7674-1cf8-46b5-f112-8df360ae6fbf","executionInfo":{"status":"ok","timestamp":1576202606712,"user_tz":300,"elapsed":736,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hkdmb4xkMbBk","colab_type":"code","outputId":"9387389c-51fb-46a9-caa0-6dc8380d1673","executionInfo":{"status":"ok","timestamp":1576202612059,"user_tz":300,"elapsed":395,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd '/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification'"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kTggmHlONzXY","colab_type":"code","outputId":"9f75f07d-9756-466f-8c7e-a5a4644920eb","executionInfo":{"status":"ok","timestamp":1576202614573,"user_tz":300,"elapsed":990,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%pwd"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Colab Notebooks/mini project 4/Sentence Pair Classification'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"KA7tGYpOMmLV","colab_type":"text"},"source":["# Data Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"KBEevuQMMpoM","colab_type":"code","outputId":"c306189d-ca42-4259-f1d8-455ed074e399","executionInfo":{"status":"ok","timestamp":1576202807534,"user_tz":300,"elapsed":15264,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import re\n","import jieba\n","import random\n","import csv\n","from tensorflow.contrib import learn\n","\n","\n","class data_prepare(object):\n","\n","    def readfile(self, filename):\n","        texta = []\n","        textb = []\n","        tag = []\n","        with open(filename) as tsv_f:\n","            reader = csv.reader(tsv_f, delimiter='\\t')\n","            for row in reader:\n","                texta.append(self.pre_processing(row[1]))\n","                textb.append(self.pre_processing(row[2]))\n","                tag.append(row[0])\n","\n","        # shuffle\n","        index = [x for x in range(len(texta))]\n","        random.shuffle(index)\n","        texta_new = [texta[x] for x in index]\n","        textb_new = [textb[x] for x in index]\n","        tag_new = [tag[x] for x in index]\n","\n","        type = list(set(tag_new))\n","        dicts = {}\n","        tags_vec = []\n","        for x in tag_new:\n","            if x not in dicts.keys():\n","                dicts[x] = 1\n","            else:\n","                dicts[x] += 1\n","            temp = [0] * len(type)\n","            temp[int(x)] = 1\n","            tags_vec.append(temp)\n","        print(dicts)\n","        return texta_new, textb_new, tags_vec\n","\n","    def pre_processing(self, text):\n","        text = re.sub('（[^（.]*）', '', text)\n","        text = ''.join([x for x in text if '\\u4e00' <= x <= '\\u9fa5'])\n","        words = ' '.join(jieba.cut(text)).split(\" \")\n","        words = [x for x in ''.join(words)]\n","        return ' '.join(words)\n","\n","    def build_vocab(self, sentences, path):\n","        lens = [len(sentence.split(\" \")) for sentence in sentences]\n","        max_length = max(lens)\n","        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n","        vocab_processor.fit(sentences)\n","        vocab_processor.save(path)\n","\n","\n","if __name__ == '__main__':\n","    data_pre = data_prepare()\n","    data_pre.readfile('dataset/sent_pair/bq/train.tsv')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.722 seconds.\n","Prefix dict has been built succesfully.\n"],"name":"stderr"},{"output_type":"stream","text":["{'0': 43054, '1': 43146}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8hvMXrlEU-rz","colab_type":"text"},"source":["# Model: Text CNN\n"]},{"cell_type":"code","metadata":{"id":"hGrsloRNWMUp","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import math\n","\n","\n","class TextCNN(object):\n","    def __init__(\n","        self, sent_len, l2_reg, vocabulary_size,  num_filters, filter_sizes, embedding_size=300, di=50,\n","        num_classes=2, num_layers=2):\n","        \"\"\"\n","        \n","\n","        :param sent_len: sentence length\n","        :param num_filters: number of filters per filter size\n","        :param filter_sizes: list of filter sizes\n","        :param w: filter width\n","        :param l2_reg: L2 regularization coefficient\n","        :param num_features: The number of pre-set features(not coming from CNN) used in the output layer.\n","        :param embedding_size: dimensionality of word embedding(default: 300)\n","        :param di: The number of convolution kernels (default: 50)\n","        :param num_classes: The number of classes for answers.\n","        :param num_layers: The number of convolution layers.\n","        \"\"\"\n","        self.text_a = tf.placeholder(tf.int32, shape=[None, sent_len], name=\"text_a\")\n","        self.text_b = tf.placeholder(tf.int32, shape=[None, sent_len], name=\"text_b\")\n","        self.y = tf.placeholder(tf.int32, shape=[None, num_classes], name=\"y\")\n","        self.dropout = tf.placeholder(tf.float32, name=\"dropout\")\n","        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n","\n","        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n","\n","        def _linear(input_, output_size, scope=\"SimpleLinear\"):\n","            shape = input_.get_shape().as_list()\n","            if len(shape) != 2:\n","                raise ValueError(\"Linear is expecting 2D arguments: {0}\".format(str(shape)))\n","            if not shape[1]:\n","                raise ValueError(\"Linear expects shape[1] of arguments: {0}\".format(str(shape)))\n","            input_size = shape[1]\n","\n","            # Now the computation.\n","            with tf.variable_scope(scope):\n","                W = tf.get_variable(\"W\", [input_size, output_size], dtype=input_.dtype)\n","                b = tf.get_variable(\"b\", [output_size], dtype=input_.dtype)\n","\n","            return tf.nn.xw_plus_b(input_, W, b)\n","\n","        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n","            \"\"\"\n","            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n","            t = sigmoid(Wy + b)\n","            z = t * g(Wy + b) + (1 - t) * y\n","            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n","            \"\"\"\n","\n","            for idx in range(num_layers):\n","                g = f(_linear(input_, size, scope=(\"highway_lin_{0}\".format(idx))))\n","                t = tf.sigmoid(_linear(input_, size, scope=(\"highway_gate_{0}\".format(idx))) + bias)\n","                output = t * g + (1. - t) * input_\n","                input_ = output\n","\n","            return output\n","\n","        # Embedding Layer\n","\n","        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n","            self.vocab_matrix = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n","                                                                stddev=1.0 / math.sqrt(embedding_size)),\n","                                            name='vacab_matrix')\n","            self.x1 = tf.nn.embedding_lookup(self.vocab_matrix, self.text_a)\n","            self.x2 = tf.nn.embedding_lookup(self.vocab_matrix, self.text_b)\n","\n","        self.expand_x1 = tf.expand_dims(self.x1, axis=-1)\n","        self.expand_x2 = tf.expand_dims(self.x2, axis=-1)\n","\n","\n","        #create a convolution and maxpool layer for each filter size\n","        pooled_output_x1 = []\n","        pooled_output_x2 = []\n","\n","        for filter_size in filter_sizes:\n","            with tf.name_scope(\"conv-filter{0}\".format(filter_size)):\n","                # Convolution Layer\n","                filter_shape = [filter_size, embedding_size, 1, num_filters]\n","                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=\"W\")\n","                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=\"b\")\n","                conv_front = tf.nn.conv2d(\n","                    self.expand_x1,\n","                    W,\n","                    strides=[1, 1, 1, 1],\n","                    padding=\"VALID\",\n","                    name=\"conv_front\")\n","\n","                conv_behind = tf.nn.conv2d(\n","                    self.expand_x2,\n","                    W,\n","                    strides=[1, 1, 1, 1],\n","                    padding=\"VALID\",\n","                    name=\"conv_behind\")\n","        \n","                # Batch Normalization Layer\n","\n","                conv_bn_x1 = tf.layers.batch_normalization(tf.nn.bias_add(conv_front, b), training=self.is_training)\n","                conv_bn_x2 = tf.layers.batch_normalization(tf.nn.bias_add(conv_behind,b), training=self.is_training)\n","\n","                # Apply nonlinearity\n","\n","                conv_out_x1 = tf.nn.relu(conv_bn_x1, name=\"relu_front\")\n","                conv_out_x2 = tf.nn.relu(conv_bn_x2, name=\"relu_behind\")\n","\n","            with tf.name_scope(\"pool-filter{0}\".format(filter_size)):\n","                # Maxpooling over the outputs\n","                pooled_front = tf.nn.max_pool(\n","                    conv_out_x1,\n","                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n","                    strides=[1, 1, 1, 1],\n","                    padding=\"VALID\",\n","                    name=\"pool_front\")\n","\n","                pooled_behind = tf.nn.max_pool(\n","                    conv_out_x2,\n","                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n","                    strides=[1, 1, 1, 1],\n","                    padding=\"VALID\",\n","                    name=\"pool_behind\")\n","\n","            pooled_outputs_front.append(pooled_front)\n","            pooled_outputs_behind.append(pooled_behind)\n","\n","        # Combine all the pooled features\n","        num_filters_total = num_filters * len(filter_sizes)\n","        self.pool_front = tf.concat(pooled_outputs_front, axis=3)\n","        self.pool_behind = tf.concat(pooled_outputs_behind, axis=3)\n","        self.pool_flat_front = tf.reshape(self.pool_front, shape=[-1, num_filters_total])\n","        self.pool_flat_behind = tf.reshape(self.pool_behind, shape=[-1, num_filters_total])\n","\n","        self.pool_flat_combine = tf.concat([self.pool_flat_front, self.pool_flat_behind], axis=1)\n","\n","        # Fully Connected Layer\n","        with tf.name_scope(\"fc\"):\n","            W = tf.Variable(tf.truncated_normal(shape=[num_filters_total * 2, fc_hidden_size],\n","                                                stddev=0.1, dtype=tf.float32), name=\"W\")\n","            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=\"b\")\n","            self.fc = tf.nn.xw_plus_b(self.pool_flat_combine, W, b)\n","\n","            # Batch Normalization Layer\n","            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n","\n","            # Apply nonlinearity\n","            self.fc_out = tf.nn.relu(self.fc_bn, name=\"relu\")\n","\n","        # Highway Layer\n","        with tf.name_scope(\"highway\"):\n","            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n","\n","        # Add dropout\n","        with tf.name_scope(\"dropout\"):\n","            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n","\n","        # Final scores and predictions\n","        with tf.name_scope(\"output\"):\n","            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n","                                                stddev=0.1, dtype=tf.float32), name=\"W\")\n","            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=\"b\")\n","            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"logits\")\n","            self.softmax_scores = tf.nn.softmax(self.logits, name=\"softmax_scores\")\n","            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n","            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=\"topKPreds\")\n","\n","        # Calculate mean cross-entropy loss, L2 loss\n","        with tf.name_scope(\"loss\"):\n","            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n","            losses = tf.reduce_mean(losses, name=\"softmax_losses\")\n","            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n","                                 name=\"l2_losses\") * l2_reg_lambda\n","            self.loss = tf.add(losses, l2_losses, name=\"loss\")\n","\n","        # Accuracy\n","        with tf.name_scope(\"accuracy\"):\n","            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n","            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n","\n","        # TODO: Reconsider the metrics calculation\n","        # Number of correct predictions\n","        with tf.name_scope(\"num_correct\"):\n","            correct = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n","            self.num_correct = tf.reduce_sum(tf.cast(correct, \"float\"), name=\"num_correct\")\n","\n","        # Calculate Fp\n","        with tf.name_scope(\"fp\"):\n","            fp = tf.metrics.false_positives(labels=tf.argmax(self.input_y, 1), predictions=self.predictions)\n","            self.fp = tf.reduce_sum(tf.cast(fp, \"float\"), name=\"fp\")\n","\n","        # Calculate Fn\n","        with tf.name_scope(\"fn\"):\n","            fn = tf.metrics.false_negatives(labels=tf.argmax(self.input_y, 1), predictions=self.predictions)\n","            self.fn = tf.reduce_sum(tf.cast(fn, \"float\"), name=\"fn\")\n","\n","        # Calculate Recall\n","        with tf.name_scope(\"recall\"):\n","            self.recall = self.num_correct / (self.num_correct + self.fn)\n","\n","        # Calculate Precision\n","        with tf.name_scope(\"precision\"):\n","            self.precision = self.num_correct / (self.num_correct + self.fp)\n","\n","        # Calculate F1\n","        with tf.name_scope(\"F1\"):\n","            self.F1 = (2 * self.precision * self.recall) / (self.precision + self.recall)\n","\n","        # Calculate AUC\n","        with tf.name_scope(\"AUC\"):\n","            self.AUC = tf.metrics.auc(self.softmax_scores, self.input_y, name=\"AUC\")\n","\n","\n","\n","# if __name__ == '__main__':\n","#     cnn = TextCNN(20, 0.001, vocabulary_size=1000, num_filters=128, filter_sizes=\"3,4,5\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5eZU_mAX_G2l","colab_type":"text"},"source":["# Hyperparameters\n"]},{"cell_type":"code","metadata":{"id":"FsXHGEZP_Nhg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a181cce4-f0ab-4285-b295-694c203a9ca9","executionInfo":{"status":"ok","timestamp":1576202846265,"user_tz":300,"elapsed":396,"user":{"displayName":"LQ Bach","photoUrl":"","userId":"00362764068960187118"}}},"source":["data_prepare.readfile"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.data_prepare.readfile>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"VaxLgMko_kyI","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import data_prepare\n","from tensorflow.contrib import learn\n","import numpy as np\n","import abcnn_mdoel\n","import abcnn_model_pre\n","import config as config\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn import metrics\n","import os\n","\n","con = config.Config()\n","parent_path = os.path.dirname(os.getcwd())\n","data_pre = data_prepare.Data_Prepare()\n","\n","\n","class TrainModel(object):\n","    def pre_processing(self):\n","        train_texta, train_textb, train_tag = data_pre.readfile(parent_path+'/data/train.txt')\n","        data = []\n","        data.extend(train_texta)\n","        data.extend(train_textb)\n","        data_pre.build_vocab(data, parent_path+'/save_model' + '/abcnn/vocab.pickle')\n","        # 加载词典\n","        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore(parent_path+'/save_model/abcnn' +\n","                                                                               '/vocab.pickle')\n","        train_texta_embedding = np.array(list(self.vocab_processor.transform(train_texta)))\n","        train_textb_embedding = np.array(list(self.vocab_processor.transform(train_textb)))\n","\n","        dev_texta, dev_textb, dev_tag = data_pre.readfile(parent_path+'/data/dev.txt')\n","        dev_texta_embedding = np.array(list(self.vocab_processor.transform(dev_texta)))\n","        dev_textb_embedding = np.array(list(self.vocab_processor.transform(dev_textb)))\n","        return train_texta_embedding, train_textb_embedding, np.array(train_tag), \\\n","               dev_texta_embedding, dev_textb_embedding, np.array(dev_tag)\n","\n","    def get_batches(self, texta, textb, tag):\n","        num_batch = int(len(texta) / con.Batch_Size)\n","        for i in range(num_batch):\n","            a = texta[i*con.Batch_Size:(i+1)*con.Batch_Size]\n","            b = textb[i*con.Batch_Size:(i+1)*con.Batch_Size]\n","            t = tag[i*con.Batch_Size:(i+1)*con.Batch_Size]\n","            yield a, b, t\n","\n","    def trainModel(self):\n","        train_texta_embedding, train_textb_embedding, train_tag, \\\n","        dev_texta_embedding, dev_textb_embedding, dev_tag = self.pre_processing()\n","\n","        # 定义训练用的循环神经网络模型\n","        # abcnn\n","        # DEFAULT_CONFIG = [{'type': 'ABCNN-1', 'w': 3, 'n': 50, 'nl': 'tanh'} for _ in range(3)]\n","        # model = abcnn_mdoel.ABCNN(True, learning_rate=con.learning_rate, conv_layers=1, embed_size=con.embedding_size,\n","        #                           vocabulary_size=len(self.vocab_processor.vocabulary_),\n","        #                           sentence_len=len(train_texta_embedding[0]), config=DEFAULT_CONFIG)\n","        model = abcnn_model_pre.ABCNN(True, len(train_texta_embedding[0]), 3, con.l2_lambda, 'ABCNN3',\n","                                      vocabulary_size=len(self.vocab_processor.vocabulary_), d0=con.embedding_size,\n","                                      di=50, num_classes=2, num_layers=1)\n","\n","        # 训练模型\n","        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n","        sess = tf.Session(config=session_conf)\n","        with sess.as_default():\n","            tf.global_variables_initializer().run()\n","            saver = tf.train.Saver()\n","            best_f1 = 0.0\n","            for time in range(con.epoch):\n","                print(\"training \" + str(time + 1) + \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n","                model.is_trainning = True\n","                loss_all = []\n","                accuracy_all = []\n","                for texta, textb, tag in tqdm(\n","                        self.get_batches(train_texta_embedding, train_textb_embedding, train_tag)):\n","                    feed_dict = {\n","                        model.text_a: texta,\n","                        model.text_b: textb,\n","                        model.y: tag\n","                    }\n","                    _, cost, accuracy = sess.run([model.train_op, model.loss, model.accuracy], feed_dict)\n","                    loss_all.append(cost)\n","                    accuracy_all.append(accuracy)\n","\n","                print(\"第\" + str((time + 1)) + \"次迭代的损失为：\" + str(np.mean(np.array(loss_all))) + \";准确率为：\" +\n","                      str(np.mean(np.array(accuracy_all))))\n","\n","                def dev_step():\n","                    \"\"\"\n","                    Evaluates model on a dev set\n","                    \"\"\"\n","                    loss_all = []\n","                    accuracy_all = []\n","                    predictions = []\n","                    for texta, textb, tag in tqdm(\n","                            self.get_batches(dev_texta_embedding, dev_textb_embedding, dev_tag)):\n","                        feed_dict = {\n","                            model.text_a: texta,\n","                            model.text_b: textb,\n","                            model.y: tag\n","                        }\n","                        dev_cost, dev_accuracy, prediction = sess.run([model.loss, model.accuracy,\n","                                                                       model.prediction], feed_dict)\n","                        loss_all.append(dev_cost)\n","                        accuracy_all.append(dev_accuracy)\n","                        predictions.extend(prediction)\n","                    y_true = [np.nonzero(x)[0][0] for x in dev_tag]\n","                    y_true = y_true[0:len(loss_all)*con.Batch_Size]\n","                    f1 = f1_score(np.array(y_true), np.array(predictions), average='weighted')\n","                    print('分类报告:\\n', metrics.classification_report(np.array(y_true), predictions))\n","                    print(\"验证集：loss {:g}, acc {:g}, f1 {:g}\\n\".format(np.mean(np.array(loss_all)),\n","                                                                      np.mean(np.array(accuracy_all)), f1))\n","                    return f1\n","\n","                model.is_trainning = False\n","                f1 = dev_step()\n","\n","                if f1 > best_f1:\n","                    best_f1 = f1\n","                    saver.save(sess, parent_path + \"/save_model/abcnn/model.ckpt\")\n","                    print(\"Saved model success\\n\")\n","\n","\n","if __name__ == '__main__':\n","    train = TrainModel()\n","    train.trainModel()"],"execution_count":0,"outputs":[]}]}